{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DATA_PATH = '../data/train'\n",
    "MODEL_SAVE_PATH = '../saved_models/deeplabv3_oil_spill.h5'\n",
    "\n",
    "# --- 1. CRITICAL FIX: DIFFERENTIABLE LOSS FUNCTIONS ---\n",
    "\n",
    "# Soft Dice Coefficient (Use this for LOSS)\n",
    "# It uses the raw probabilities (0.0 to 1.0) instead of thresholding.\n",
    "# This allows the gradient to flow back through the network.\n",
    "def dice_coeff_soft(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Dice Loss function (Minimize this)\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coeff_soft(y_true, y_pred)\n",
    "\n",
    "# --- METRICS (For Human Monitoring) ---\n",
    "# It is okay to use thresholding here because metrics are not used for backpropagation.\n",
    "\n",
    "# Intersection over Union (IoU)\n",
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred_binary = K.cast(K.greater(y_pred, 0.5), K.floatx())\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred_binary)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "# Hard Dice Score (F1 Score) - Strictly for reporting accuracy\n",
    "def dice_metric_hard(y_true, y_pred, smooth=1e-6):\n",
    "    y_pred_binary = K.cast(K.greater(y_pred, 0.5), K.floatx())\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred_binary)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# --- 2. DATA LOADER ---\n",
    "def load_data(path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    img_dir = os.path.join(path, 'images')\n",
    "    mask_dir = os.path.join(path, 'labels') \n",
    "\n",
    "    if not os.path.exists(mask_dir):\n",
    "        mask_dir = os.path.join(path, 'masks') \n",
    "\n",
    "    files = os.listdir(img_dir)[:500] # Loading first 500 for training\n",
    "    for file_name in files:\n",
    "        img_path = os.path.join(img_dir, file_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "\n",
    "        mask_path = os.path.join(mask_dir, file_name)\n",
    "        if not os.path.exists(mask_path):\n",
    "            mask_path = os.path.join(mask_dir, os.path.splitext(file_name)[0] + \".png\")\n",
    "            \n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        if mask is None: continue\n",
    "\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    images = np.array(images) / 255.0\n",
    "    masks = np.array(masks) / 255.0\n",
    "    masks = np.expand_dims(masks, axis=-1)\n",
    "    return images, masks\n",
    "\n",
    "# Load and Split\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    X, y = load_data(DATA_PATH)\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(f\"No images found in {DATA_PATH}. Check your paths.\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Loaded {len(X)} images successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR during data loading: {e}\")\n",
    "    X_train = None # Prevent subsequent code from crashing blindly\n",
    "\n",
    "# --- 3. MODEL ARCHITECTURE ---\n",
    "def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n",
    "    x = Conv2D(num_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding=\"same\", use_bias=use_bias)(block_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation(\"relu\")(x)\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "    \n",
    "    x_pool = AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x_pool = convolution_block(x_pool, kernel_size=1, use_bias=True)\n",
    "    \n",
    "    # Robust upsampling calculation\n",
    "    pool_shape = K.int_shape(x_pool)\n",
    "    target_h = dims[1]\n",
    "    target_w = dims[2]\n",
    "    \n",
    "    # Calculate scale factors dynamically\n",
    "    if target_h is not None and pool_shape[1] is not None:\n",
    "        scale_h = target_h // pool_shape[1]\n",
    "        scale_w = target_w // pool_shape[2]\n",
    "        x_pool = UpSampling2D(size=(scale_h, scale_w), interpolation=\"bilinear\")(x_pool)\n",
    "    \n",
    "    x = concatenate([x, out_6, out_12, out_18, x_pool])\n",
    "    return x\n",
    "\n",
    "def improved_deeplabv3_plus(input_shape, n_classes=1):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder: MobileNetV2\n",
    "    mobilenet = MobileNetV2(input_shape=input_shape, \n",
    "                            include_top=False, \n",
    "                            weights=\"imagenet\", \n",
    "                            input_tensor=inputs) \n",
    "    \n",
    "    # Feature extraction points\n",
    "    high_level_features = mobilenet.get_layer(\"block_13_expand\").output\n",
    "    low_level_features = mobilenet.get_layer(\"block_3_expand\").output \n",
    "    \n",
    "    # ASPP\n",
    "    x = DilatedSpatialPyramidPooling(high_level_features)\n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "    \n",
    "    # Low-level features processing\n",
    "    low_level_conv = Conv2D(48, (1, 1), padding=\"same\", use_bias=False)(low_level_features)\n",
    "    low_level_conv = BatchNormalization()(low_level_conv)\n",
    "    low_level_conv = Activation(\"relu\")(low_level_conv)\n",
    "\n",
    "    # Decoder\n",
    "    x = concatenate([x, low_level_conv])\n",
    "    x = convolution_block(x, 256)\n",
    "    x = convolution_block(x, 256)\n",
    "\n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# --- 4. TRAINING ---\n",
    "if X_train is not None:\n",
    "    # Build Model\n",
    "    model = improved_deeplabv3_plus((IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    # Compile with the NEW differentiable loss\n",
    "    print(\"Compiling model with Soft Dice Loss...\")\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss=dice_loss, \n",
    "                  metrics=['accuracy', iou_metric, dice_metric_hard])\n",
    "    \n",
    "    # model.summary() # Optional: Uncomment to see architecture\n",
    "\n",
    "    print(\"Starting DeepLab Training...\")\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=30, \n",
    "                        validation_data=(X_val, y_val))\n",
    "    \n",
    "    # Save Model\n",
    "    model.save(MODEL_SAVE_PATH)\n",
    "    print(f\"DeepLab model saved to {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    # Plot Results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss (Soft Dice)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['dice_metric_hard'], label='Train F1 (Dice)')\n",
    "    plt.plot(history.history['val_dice_metric_hard'], label='Val F1 (Dice)')\n",
    "    plt.title('Accuracy (Hard Dice/F1)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping training due to data loading errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
